{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7031,"status":"ok","timestamp":1728635480710,"user":{"displayName":"data","userId":"05498871407802855155"},"user_tz":-540},"id":"APIgr6LQ5ONI"},"outputs":[],"source":["import os\n","from PIL import Image\n","\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","import torchvision.transforms as transforms\n","\n","from torch.utils.data import DataLoader, Dataset, ConcatDataset, WeightedRandomSampler, Subset\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":465,"status":"ok","timestamp":1728635497732,"user":{"displayName":"data","userId":"05498871407802855155"},"user_tz":-540},"id":"rhgtNocW5VRA","outputId":"70ee42f4-7e33-4570-b38d-7c6b3682baa2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}],"source":["device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728635499504,"user":{"displayName":"data","userId":"05498871407802855155"},"user_tz":-540},"id":"js8NxZ9K6B0b"},"outputs":[],"source":["def conv3x3(in_planes, out_planes, stride=1, padding=1, bias=False):\n","  return nn.Conv2d(in_planes, out_planes,\n","      kernel_size =3,\n","      stride = stride,\n","      padding = padding,\n","      bias = bias\n","  )\n","\n","def conv1x1(in_planes, out_planes, stride=1, padding=0, bias=False):\n","  return nn.Conv2d(in_planes, out_planes,\n","      kernel_size =1,\n","      stride = stride,\n","      padding = padding,\n","      bias = bias\n","  )"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1728635501075,"user":{"displayName":"data","userId":"05498871407802855155"},"user_tz":-540},"id":"hkZ2JVzX6Ijz"},"outputs":[],"source":["# ResNet 18, 34에 쓰이는 Basic Block\n","class BasicBlock(nn.Module):\n","  mul = 1\n","  def __init__(self, in_planes, out_planes, stride=1):\n","    super(BasicBlock, self).__init__()\n","\n","    self.conv1 = conv3x3(in_planes, out_planes, stride)\n","    self.conv2 = conv3x3(out_planes, out_planes, 1)\n","\n","    self.bn1 = nn.BatchNorm2d(out_planes)\n","    self.bn2 = nn.BatchNorm2d(out_planes)\n","\n","    self.shortcut = nn.Sequential()\n","    if stride != 1:\n","      self.shortcut = nn.Sequential(\n","          conv1x1(in_planes, out_planes, stride),\n","          nn.BatchNorm2d(out_planes)\n","      )\n","\n","  def forward(self, x):\n","    out = self.conv1(x)\n","    out = self.bn1(out)\n","    out = F.relu(out)\n","    out = self.conv2(out)\n","    out = self.bn2(out)\n","    out += self.shortcut(x)\n","    out = F.relu(out)\n","    return out\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":506,"status":"ok","timestamp":1728635503267,"user":{"displayName":"data","userId":"05498871407802855155"},"user_tz":-540},"id":"NE2ht23U6KOw"},"outputs":[],"source":["class ResNet(nn.Module):\n","  def __init__(self, block, num_blocks, num_classes=31):\n","    super(ResNet, self).__init__()\n","\n","    # 7*7, 64 channels, stride 2 in paper\n","    self.in_planes = 64\n","\n","    # RGB channel -> 64 channels\n","    self.conv = nn.Conv2d(3, self.in_planes, kernel_size=7, stride=2, padding=3)\n","    self.bn = nn.BatchNorm2d(self.in_planes)\n","    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","    # self.gender_fc = nn.Linear(num_features, 2)  # 성별 분류\n","    # self.style_fc = nn.Linear(num_features, 31)  # 스타일 분류\n","\n","    _layers = []\n","    outputs, strides = [64, 128, 256, 512], [1, 2, 2, 2]\n","    for i in range(4):\n","      _layers.append(self._make_layer(block, outputs[i], num_blocks[i], stride=strides[i]))\n","    self.layers = nn.Sequential(*_layers)\n","\n","    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","    self.linear = nn.Linear(512 * block.mul, num_classes)\n","\n","  def _make_layer(self, block, out_planes, num_block, stride):\n","    layers = [ block(self.in_planes, out_planes, stride) ]\n","    self.in_planes = block.mul * out_planes\n","    for i in range(num_block - 1):\n","      layers.append(block(self.in_planes, out_planes, 1))\n","\n","    return nn.Sequential(*layers)\n","\n","  def forward(self, x):\n","    out = self.conv(x)\n","    out = self.bn(out)\n","    out = F.relu(out)\n","    out = self.maxpool(out)\n","\n","    out = self.layers(out)\n","    out = self.avgpool(out)\n","    out = out.view(out.size(0), -1)\n","    out = self.linear(out)\n","\n","    return out"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":472,"status":"ok","timestamp":1728635506528,"user":{"displayName":"data","userId":"05498871407802855155"},"user_tz":-540},"id":"L4_-n8zv6MGG"},"outputs":[],"source":["def ResNet18():\n","  return ResNet(BasicBlock, [2, 2, 2, 2])"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":507,"status":"ok","timestamp":1728635508451,"user":{"displayName":"data","userId":"05498871407802855155"},"user_tz":-540},"id":"OrSV41cH6Nj1"},"outputs":[],"source":["# 남성 스타일과 여성 스타일을 성별 구분하여 인덱싱\n","style_dict = {\n","    # 남성 스타일 (8개)\n","    'M_bold': 0,\n","    'M_hiphop': 1,\n","    'M_hippie': 2,\n","    'M_ivy': 3,\n","    'M_metrosexual': 4,\n","    'M_mods': 5,\n","    'M_normcore': 6,\n","    'M_sportivecasual': 7,\n","\n","    # 여성 스타일 (23개)\n","    'W_athleisure': 8,\n","    'W_bodyconscious': 9,\n","    'W_cityglam': 10,\n","    'W_classic': 11,\n","    'W_disco': 12,\n","    'W_ecology': 13,\n","    'W_feminine': 14,\n","    'W_genderless': 15,\n","    'W_grunge': 16,\n","    'W_hiphop': 17,\n","    'W_hippie': 18,\n","    'W_kitsch': 19,\n","    'W_lingerie': 20,\n","    'W_lounge': 21,\n","    'W_military': 22,\n","    'W_minimal': 23,\n","    'W_normcore': 24,\n","    'W_oriental': 25,\n","    'W_popart': 26,\n","    'W_powersuit': 27,\n","    'W_punk': 28,\n","    'W_space': 29,\n","    'W_sportivecasual': 30\n","}\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728635510125,"user":{"displayName":"data","userId":"05498871407802855155"},"user_tz":-540},"id":"eN5ftKQP6O9t"},"outputs":[],"source":["class ImageDataset(Dataset):\n","  def __init__(self, image_dir, transform=None):\n","    self.image_dir = image_dir\n","    self.image_files = [ f for f in os.listdir(image_dir) if f.endswith('.jpg') and os.path.isfile(os.path.join(image_dir, f))]\n","    self.transform = transform\n","\n","  def __len__(self):\n","    return len(self.image_files)\n","\n","  def __getitem__(self, idx):\n","    img_name = self.image_files[idx]\n","    img_path = os.path.join(self.image_dir, img_name)\n","    image = Image.open(img_path).convert('RGB')  # RGB로 변환\n","\n","    # 파일명에서 성별과 스타일 추출\n","    parts = img_name.split('_')\n","    try:\n","        gender = parts[-1].replace('.jpg', '')  # 성별\n","        style = parts[-2]  # 스타일\n","    except IndexError:\n","        print(f\"Error parsing filename: {img_name}\")\n","        return None, None  # or handle the error differently\n","\n","    label = f\"{gender}_{style}\"\n","\n","    # 라벨을 (성별, 스타일)로 설정 (성별을 0/1, 스타일을 별도의 인덱스로 설정)\n","    # gender_label = 0 if gender == 'M' else 1\n","    style_label = style_dict.get(f\"{gender}_{style}\")\n","\n","    if self.transform:\n","      image = self.transform(image)\n","\n","    return image, style_label"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q5t5bx2w6QvB","outputId":"bfe5f026-c85e-4415-9482-08d5f5aeb0dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","train_image_dir = '/content/drive/MyDrive/dataset/training_image'\n","val_image_dir = '/content/drive/MyDrive/dataset/validation_image'\n","\n","resnet_transform = transforms.Compose([\n","    transforms.Resize((224, 224)), # resnet paper input\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# transform_augmentation = transforms.Compose([\n","#     transforms.Resize((224, 224)),\n","#     transforms.CenterCrop((224,224)),\n","#     transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n","#     transforms.RandomHorizontalFlip(),\n","#     transforms.RandomRotation(degrees=15),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","# ])\n","center_crop_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.CenterCrop((224, 224))\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","])\n","\n","train_dataset = ImageDataset(train_image_dir, transform=resnet_transform)\n","\n","# class_indices = [[] for _ in range(31)]\n","# for i in range(len(train_dataset)):\n","#   _, label = train_dataset[i]\n","#   class_indices[label].append(i)\n","\n","# calculate the number of examples to sample from each class\n","# max_class_size = max([len(class_indices[c]) for c in range(31)])\n","# class_weights = [max_class_size / len(class_indices[c]) for c in range(31)]\n","# num_samples = [int(class_weights[c] * len(class_indices[c])) for c in range(31)]\n","\n","# create a WeightedRandomSampler to oversample the training set\n","# sampler = WeightedRandomSampler(weights=class_weights, num_samples=sum(num_samples), replacement=True)\n","\n","train_crop = ImageDataset(train_image_dir, center_crop_transform)\n","train_dataset = ConcatDataset([train_dataset, train_crop])\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, sampler=sampler) # 현재 논문상에서 256 배치사이즈를 사용했다고 함\n","\n","#create new training set with oversampled examples\n","# oversampled_train_dataset = Subset(train_dataset, indices=list(sampler))\n","\n","# Sampling the subset\n","# oversampled_train_dataset.transform = transform_augmentation\n","# subset_train_loader = train_loader = DataLoader(oversampled_train_dataset, batch_size=64, sampler=sampler)\n","# 이부분은 추후 실험을 통해서 조정을 해야할듯.\n","\n","# train_dataset = ConcatDataset([train_dataset, oversampled_train_dataset])\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, sampler=sampler) # 현재 논문상에서 256 배치사이즈를 사용했다고 함\n","\n","\n","val_dataset = ImageDataset(val_image_dir, transform=resnet_transform)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","print(f\"{len(train_dataset)} train dataset 길이\")\n","# print(f\"{len(oversampled_train_dataset)} train dataset 길이\")\n","print(f\"{len(val_dataset)} validation dataset 길이\")"]},{"cell_type":"code","source":[],"metadata":{"id":"g5ftHsA1O_V5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":532,"status":"ok","timestamp":1728633062625,"user":{"displayName":"data","userId":"05498871407802855155"},"user_tz":-540},"id":"875nxV0E6SfJ"},"outputs":[],"source":["def evaluate(model, val_loader, criterion):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    total_loss = 0.0\n","\n","    with torch.no_grad():\n","        for batch_in, batch_out in val_loader:\n","            batch_in = batch_in.to(device)\n","            batch_out = batch_out.to(device)\n","            # target = batch_out.view(-1)\n","            outputs = model(batch_in)\n","            loss = criterion(outputs, batch_out)\n","            total_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            correct += (predicted == batch_out).sum().item()\n","            total += batch_out.size(0)\n","    accuracy = correct / total\n","    avg_loss = total_loss / len(val_loader)\n","    # for inputs, labels in val_loader:\n","    #     with torch.no_grad():\n","            # outputs = model(inputs.to(device))\n","            # loss = criterion(outputs, labels.to(device))\n","\n","            # total_loss += loss.item()\n","            # _, predicted = torch.max(outputs, 1)\n","            # correct += (predicted == labels.to(device)).sum().item()\n","            # total += labels.size(0)\n","\n","    # accuracy = correct / total if total > 0 else 0\n","    # avg_loss = total_loss / len(val_loader) if len(val_loader) > 0 else 0\n","\n","    return accuracy, avg_loss"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"XvOI_rOr89AC","executionInfo":{"status":"error","timestamp":1728634174772,"user_tz":-540,"elapsed":1110395,"user":{"displayName":"data","userId":"05498871407802855155"}},"outputId":"7b942bcc-3ae5-4805-dc9c-1043739c3e8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 3.6024\n","Epoch [1/10], Loss: 4.0140\n","Epoch [1/10], Loss: 4.2099\n","Epoch [1/10], Loss: 4.0249\n","Epoch [1/10], Loss: 3.5301\n","Epoch [1/10], Loss: 3.2848\n","Epoch [1/10], Loss: 3.4614\n","Epoch [1/10], Loss: 3.4896\n","Epoch [1/10], Loss: 3.6256\n","Epoch [1/10], Loss: 3.4224\n","Epoch [1/10], Loss: 3.3265\n","Epoch [1/10], Loss: 3.5610\n","Epoch [1/10], Loss: 3.4043\n","Epoch [1/10], Loss: 3.3171\n","Epoch [1/10], Loss: 3.4513\n","Epoch [1/10], Loss: 3.5223\n","Epoch [1/10], Loss: 3.2885\n","Epoch [1/10], Loss: 3.5071\n","Epoch [1/10], Loss: 3.2822\n","Epoch [1/10], Loss: 3.3951\n","Epoch [1/10], Loss: 3.6531\n","Epoch [1/10], Loss: 3.3944\n","Epoch [1/10], Loss: 3.4382\n","Epoch [1/10], Loss: 3.2250\n","Epoch [1/10], Loss: 3.3126\n","Epoch [1/10], Loss: 3.3880\n","Epoch [1/10], Loss: 3.3937\n","Epoch [1/10], Loss: 3.5647\n","Epoch [1/10], Loss: 3.2431\n","Epoch [1/10], Loss: 3.4455\n","Epoch [1/10], Loss: 3.5067\n","Epoch [1/10], Loss: 3.3445\n","Epoch [1/10], Loss: 3.5228\n","Epoch [1/10], Loss: 3.2232\n","Epoch [1/10], Loss: 3.4072\n","Epoch [1/10], Loss: 3.3198\n","Epoch [1/10], Loss: 3.4364\n","Epoch [1/10], Loss: 3.3835\n","Epoch [1/10], Loss: 3.3471\n","Epoch [1/10], Loss: 3.3643\n","Epoch [1/10], Loss: 3.2129\n","Epoch [1/10], Loss: 3.4567\n","Epoch [1/10], Loss: 3.1890\n","Epoch [1/10], Loss: 3.5895\n","Epoch [1/10], Loss: 3.3320\n","Epoch [1/10], Loss: 3.3507\n","Epoch [1/10], Loss: 3.2590\n","Epoch [1/10], Loss: 3.3810\n","Epoch [1/10], Loss: 3.3989\n","Epoch [1/10], Loss: 3.2521\n","Epoch [1/10], Loss: 3.4433\n","Epoch [1/10], Loss: 3.3357\n","Epoch [1/10], Loss: 3.4824\n","Epoch [1/10], Loss: 3.3223\n","Epoch [1/10], Loss: 3.1973\n","Epoch [1/10], Loss: 3.2289\n","Epoch [1/10], Loss: 3.1719\n","Epoch [1/10], Loss: 3.3242\n","Epoch [1/10], Loss: 3.4700\n","Epoch [1/10], Loss: 3.3472\n","Epoch [1/10], Loss: 3.3991\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-ce26f9471bfe>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-0db452a1871b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# RGB로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# 파일명에서 성별과 스타일 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m                         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodermaxblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                                 \u001b[0;31m# truncated png/gif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36mload_read\u001b[0;34m(self, read_bytes)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mso\u001b[0m \u001b[0mlibjpeg\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mfinish\u001b[0m \u001b[0mdecoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \"\"\"\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOAD_TRUNCATED_IMAGES\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_ended\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model = ResNet18().to(device)\n","\n","epochs = 10\n","\n","criterion = nn.CrossEntropyLoss()  # loss func 정의\n","\n","# 논문에서 제시한 optim 사용\n","optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-4)\n","\n","for epoch in range(epochs):\n","  model.train()\n","  running_loss = 0.0\n","  for inputs, labels in train_loader:\n","    optimizer.zero_grad()\n","    outputs = model(inputs.to(device))\n","\n","    loss = criterion(outputs, labels.to(device))\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    running_loss += loss.item()\n","    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n","\n","  print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n","  accuracy, avg_loss = evaluate(model, val_loader, criterion)\n","  print(f\"Epoch {epoch+1}/{epochs}, Accuracy: {accuracy:.4f}, Avg Loss: {avg_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pheb1Krn8_Qf"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPn8KDxOLG4AyVEjRbFeBEa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}